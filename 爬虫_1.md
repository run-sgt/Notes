## 爬虫介绍

模拟浏览器向后端发送请求，获取数据，解析并获得目标数据，然后存储

通过代码手动获取数据价值

发送请求-获得数据-解析数据-存储数据

使用的工具模块bs4  pyquery  re

## 爬数据需要掌握的基础知识

### http协议

**请求**:

```
URL：指明了我要取哪里
```

```
method：
get 传递数据：？&拼在url后面
post 请求体数据类型：formdata files json
```

**请求头**：

```
Cookie：验证登录状态
```

```
Referer：告诉服务器你从哪里来
```

```
User-Agent:告诉服务器你的身份
```

**响应：**

```
Status Code
2xx
3xx
4xx
5xx
```

**响应头:**

```
location 跳转地址
set_cookie 设置cookie
```

**响应体**：

```
html代码
二进制：图片，视频，音乐
json格式
```

## 常用请求库、解析库、数据库的用法

### 常用解析语法

##### css选择器

**类选择器**

**id选择器**

**标签选择器**

**后代选择器**( )

**子选择器**(>)

**属性选择器**([属性名])5种

```
1.[属性名1] 所有包含属性名1的标签
2.[属性名='xxx'] 所以属性名为'xxx'的标签
	这里需要注意：<div class='btn form-control'></div>
	如果是[class='btn'] 此时是不会选择上面div标签的，这种方法必须完全匹配到class属性值
	如果是[class='btn form-control']就能找到该div标签。
3.[属性名2^='xxx'] 所有属性名2的值以'xxx'开头的标签
4.[属性名3$='xxx'] 所有属性名3的值以'xxx'结尾的标签
5.[属性名4*='xxx'] 所有属性名4的值包含'xxx'的标签   
```

**群组选择器**(.xxx,id_name)

**多条件选择器**(.xxx#id_name)

##### xpath选择器

待了解...

### 常用请求库

测试网站：http://httpbin.org/get

#### requests库

**安装**

```
pip3 install requests
使用时导入模块 import requests
```

**get请求：**

```
响应对象 = requests.get(url='')
参数 
	url:http://...一般会自动跳转到https://
	headers = {} 优先级高于直接在get()中写的headers参数
	cookies = {} 
	params = {}
	proxies = {'http': 'http://ip:port...'} 代理
	timeout = 0.5 超时时间
	allow_redirects = True 允许跳转，默认是True
```

**post请求：**

```
响应对象 = requests.post(url='')
参数 
	url:
	headers = {} 优先级高于直接在get()中写的headers参数
	cookies = {} 
	data = {}
	json = {}
	files = {‘file’：open（...，‘rb’）}
	timeout = 0.5 超时时间
	allow_redirects = True 允许跳转，默认是True
```

**响应对象的方法**

```
r.url
r.text 获取相应对象中的文本内容
r.encoding = 'gbk' 设置编码格式
r.content 获取响应对象中的二进制数据
r.json() 将取到的响应对象转为json格式
r.status_code 获取状态码
r.headers
r.cookies 当前响应对象中的cookie
r.history 获取跳转之前的响应对象列表[<Response[302]>]
```

**获取页面的编码格式**

```
获取获取的数据编码格式的方法
1.在请求的html页面也就是使用Google Chrome浏览器右键检查，打开控制台console，输入document.charset
2.将响应对象点content得到页面的二进制流数据，直接用with open wb方法将其直接保存为一个txt文件，然后pycharm打开，如果不是'UTF-8'编码格式怎会提示正确的编码格式。
```

**请求保存cookie再请求带上cookie：**

```
session = requests.session（）
每次发请求都会把相应的cookie自动保存
r = session.get(......)
r = session.post(......)

补充:(保存cookie到本地)
过程示例：
假设我们使用requests模块给'https://www.chiphell.com/'发送请求
服务器会给我返回cookies，我使用LWPCookieJar将cookies先保存到cookie文件中(保存的时候会自动创建该文件)
from http.cookiejar import LWPCookieJar
headers = {
'user-agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36'
}
session = requests.session()
session.cookies = LWPCookieJar(filename='cookies')
r = session.get(url='https://www.chiphell.com/', headers=headers)
session.cookies.save(filename='cookies')

此时cookie文件中就会存有该网站返回的cookies数据
然后我们在下一次请求的时候通过load方法带上cookies

session.cookies = LWPCookieJar(filename='cookies')
session.cookies.load(filename='cookies')
r = session.get(url='https://www.chiphell.com/', headers=headers)
```

---

#### `requests-html`库

**安装**

```
pip3 install requests-html
```

**`requests-html`有2种操作方式一种是HTML一种是`HTMLSession`，可以达到相同的目录，使用场景会有细微差别，看个人爱好选择使用**

##### 1、`HTML`方法

```
from requests_html import HTML
当我们能拿到相应的html文本的时候可以使用HTML实例化html对象的时候把html文本传进去
这样我们就可以使用这个html对象进行查找、渲染等操作
方法等同于r = session.request(....) 里的r.html

html = html(html=response.text)
html.find(...)
html.render(reload=False...)  # reload=False让它从内存中加载
```

##### 2、`HTMLSession`方法

**请求**

```
from requests_html import HTMLSession
session = HTMLSession()
参数：(与requests模块一模一样)

响应对象 = session.request(...)
响应对象 = session.get(...)
响应对象 = session.post(...)
```

**响应**

```
r = session.get(...)
r.url
属性与requests模块一模一样
```

```
from requests_html import HTML
html = HTML(html='')   这个html就是r.html
```

**解析**

**html对象属性**

```
r.html html对象，用来负责解析
例如：
r.html.links 所有链接(原样)
r.html.base_url 所有基础链接
r.html.absolute_links 将所有的相对链接变成绝对链接(集合、去重功能)
另一种使用方法：取出所有a标签的绝对链接
r1 = r.html.find('dd a') 取出所有a标签的绝对链接
for r in r1:
    print(list(r.absolute_links)[0])
    
基础链接 + 相对链接 = 绝对链接
r.html.html 原样html文件
r.html.encoding = 'gbk'
r.html.text 页面所有文本信息
r.html.raw_html 二进制数据流
r.html.pq 将原html转换成pyQuery对象
```

**tips**

r.encoding控制的是r.text,而r.html.encoding控制的是r.html.html

**html对象方法**

```
r.html.find('css选择器')
	可以接收两个参数,一个是css选择器，一个是first=True表示只选择第一条数据
find到的对象元素对象也有方法:
	element = r.html.find('div',first=True)
	element.base_url 当前页面的基础链接
	element.absolute_links 查找的a标签的绝对链接
	element.find(..) 还可以用find继续找
	element.raw_html 找到的内容的二进制流数据
	element.text 找到的所有文本，不包含文本以外的内容，就是一行接着一行
	element.full_text 找到的所有文本 包含换行空格元素，会存在空行
	element.attrs 这个很重要的方法，找到的标签的所有属性，给你一个字典

r.html.search('模板') 搜索元素的文本内容
	('xxx{}yyy{}')[0]
	('xxx{name}yyy{pwd}')['name']
r.html.search_all('模板')
r.render(...) 调用浏览器内核对页面进行渲染

```

参数

```
js注入：
script:'''() => {
	js代码
	js代码
}'''
scrolldown:
	r.html.render(scrolldown=100, sleep=0.1) 翻100页，每翻一页停0.1秒
sleep:
keep_page:True/False js注入之后弹出的页面关闭，r.html.page就会变成None要想拿到该页面，
	将keep_page设为True，默认是False。
	实际上将keep_page设为True后，我们才能与浏览器进行交互，比如输入、截屏、按键...
```

**让headless变成headers,让render时候弹出浏览器**

```
session = HTMLSession(
	brower_args=[
	'--no-sand',
	'--user-agent=Mozilla/5.0........'
	], headless=False
)
```

```
HTMLSession>BaseSession>async def browser函数下的pyppeteer.launch修改
参数：headless=self.headless
在上面的__init__中参数添加headless=True
下面添加self.headless = headless
```

##### 一种反爬手段

一般正常使用浏览器访问页面的时候，在后台console输入navigator.webdriver后结果为undefined

但是如果是爬虫去访问服务器时候，页面console的navigator.webdriver结果true，这就和浏览器有区别了，所以需要写一段代码来进行伪装

```
scripts = '''
    ()=>{
    Object.defineProperties(navigator,{
            webdriver:{
            get: () => undefined
            }
        })}
'''
然后在render中添加参数script=scripts
```

**使用session.browser.close()来关闭抛异常的浏览器进程**

```
r = session.get(url='...')
try:
	r.html.render(script=scripts, sleep=1, keep_page=True)
finally:
	session.browser.close()
```

对浏览器进行操作时候需要开启协程：

```
import asyncio
# 开启协程
async def main():
	await r.html.page.screenshot({'path': '1.png'})
# 执行方法一：(通过asyncio)
asyncio.get_event_loop().run_until_complete(main())
# 执行方法二：(通过session.loop)
seesion.loop.run_until_complete(main())
```

举个完全的例子：(已经通过修改源码，达到去除无头的效果，也就是运行会正常弹出浏览器)

```
from requests_html import HTMLSession
session = HTMLSession(
    browser_args=[
        '--no-sand',
        '--user-agent=Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36'
    ], headless=False
)
url = 'https://www.csdn.net/'
r = session.request(method='get', url=url)
try:
    r.html.render(scrolldown=5, sleep=0.1, keep_page=True)
    async def main():
    # 截图简洁操作
        await r.html.page.screenshot({'path': '1.png'})
    session.loop.run_until_complete(main())
finally:
    session.close()
```

**截图**

```
r.html.render(keep_page=True)
    async def main():
        await r.html.page.screenshot({'path': '3.png', 'clip': {'x': 0, 'y': 543,
                                      'width': 328, 'height': 138}})
    session.loop.run_until_complete(main())                                  
```

**evaluate** （js注入)

```
try:
    r.html.render(keep_page=True)
    async def main():
        res = await r.html.page.evaluate('''
            ()=>{
                var a = document.querySelector('#mn_N6666')
                return {'x': a.offsetLeft, 'y': a.offsetTop}
            }
        ''')
        print(res)
    session.loop.run_until_complete(main())
finally:
    session.close()
```

